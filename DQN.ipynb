{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4037eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04dd58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: 6\n",
      "Observation shape: (4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b9cbc069210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "env = AtariPreprocessing(env, grayscale_obs=True, scale_obs=True, frame_skip=4)\n",
    "env = FrameStackObservation(env, 4)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action space:\", n_actions)\n",
    "obs_shape = env.observation_space.shape\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "\n",
    "SEED = 42\n",
    "MAX_STEPS = 20000000\n",
    "REPLAY_SIZE = 100000\n",
    "WARMUP_STEPS = 10000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 1\n",
    "LR = 1e-4\n",
    "TARGET_UPDATE_FREQ = 10000 \n",
    "UPDATE_FREQ = 4\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.02\n",
    "EPS_DECAY = 0.999995\n",
    "SAVE_PATH = \"./model_checkpoints\"\n",
    "EVAL_EVERY = 100000\n",
    "EVAL_EPISODES = 10\n",
    "trained = False\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d3b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.out = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0493533",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = np.stack([b.state for b in batch])\n",
    "        actions = np.array([b.action for b in batch], dtype=np.int64)\n",
    "        rewards = np.array([b.reward for b in batch], dtype=np.float32)\n",
    "        next_states = np.stack([b.next_state for b in batch])\n",
    "        dones = np.array([b.done for b in batch], dtype=np.uint8)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    \"\"\"\n",
    "    Acepta la observación que devuelve env.reset() o env.step()\n",
    "    y devuelve un array float32 con shape (C,H,W) y valores en [0,1].\n",
    "    Maneja (H,W,4) y (4,H,W).\n",
    "    \"\"\"\n",
    "    arr = np.array(obs)\n",
    "    if arr.ndim == 3 and arr.shape[2] == 4:  # (H,W,4)\n",
    "        arr = np.transpose(arr, (2,0,1))\n",
    "    elif arr.ndim == 3 and arr.shape[0] == 4:\n",
    "        pass  # ya está (4,H,W)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Forma inesperada de observación: {arr.shape}\")\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def to_tensor(x, dtype=torch.float32):\n",
    "    return torch.from_numpy(x).to(device, dtype=dtype)\n",
    "\n",
    "if len(obs_shape) == 3:\n",
    "    if obs_shape[2] == 4:\n",
    "        in_ch = 4\n",
    "    elif obs_shape[0] == 4:\n",
    "        in_ch = 4\n",
    "    else:\n",
    "        raise RuntimeError(\"No pude inferir canales de la observación.\")\n",
    "else:\n",
    "    raise RuntimeError(\"Observación con dimensionalidad inesperada.\")\n",
    "\n",
    "policy_net = DQN(in_ch, n_actions).to(device)\n",
    "target_net = DQN(in_ch, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "replay = ReplayBuffer(REPLAY_SIZE)\n",
    "\n",
    "def compute_td_loss(batch_size):\n",
    "    states, actions, rewards, next_states, dones = replay.sample(batch_size)\n",
    "    states_v = to_tensor(states)\n",
    "    next_states_v = to_tensor(next_states)\n",
    "    actions_v = torch.from_numpy(actions).to(device, dtype=torch.int64).unsqueeze(1)\n",
    "    rewards_v = torch.from_numpy(rewards).to(device)\n",
    "    dones_v = torch.from_numpy(dones).to(device, dtype=torch.uint8)\n",
    "\n",
    "    q_values = policy_net(states_v).gather(1, actions_v).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states_v).max(1)[0]\n",
    "        next_q_values = next_q_values * (1 - dones_v)\n",
    "        expected_q = rewards_v + GAMMA * next_q_values\n",
    "\n",
    "    loss = F.smooth_l1_loss(q_values, expected_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rellenando replay buffer con acciones aleatorias...\n",
      "Replay buffer listo: 10000 transiciones. Comenzando entrenamiento...\n",
      "Episode 10 | Step 9661 | Episodic reward: -21.0 | eps=0.953 | mean100=-19.60 | replay=19661\n",
      "[Paso 10000] Copiado a target network.\n",
      "Episode 20 | Step 18625 | Episodic reward: -20.0 | eps=0.911 | mean100=-19.95 | replay=28625\n",
      "[Paso 20000] Copiado a target network.\n",
      "Episode 30 | Step 27837 | Episodic reward: -21.0 | eps=0.870 | mean100=-20.17 | replay=37837\n",
      "[Paso 30000] Copiado a target network.\n",
      "Episode 40 | Step 37000 | Episodic reward: -21.0 | eps=0.831 | mean100=-20.18 | replay=47000\n",
      "[Paso 40000] Copiado a target network.\n",
      "Episode 50 | Step 46157 | Episodic reward: -21.0 | eps=0.794 | mean100=-20.18 | replay=56157\n",
      "[Paso 50000] Copiado a target network.\n",
      "Episode 60 | Step 54782 | Episodic reward: -20.0 | eps=0.760 | mean100=-20.23 | replay=64782\n",
      "[Paso 60000] Copiado a target network.\n",
      "Episode 70 | Step 64146 | Episodic reward: -20.0 | eps=0.726 | mean100=-20.17 | replay=74146\n",
      "[Paso 70000] Copiado a target network.\n",
      "Episode 80 | Step 73273 | Episodic reward: -20.0 | eps=0.693 | mean100=-20.21 | replay=83273\n",
      "[Paso 80000] Copiado a target network.\n",
      "Episode 90 | Step 82733 | Episodic reward: -20.0 | eps=0.661 | mean100=-20.18 | replay=92733\n",
      "[Paso 90000] Copiado a target network.\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # --- Entrenamiento principal ---\n",
    "    step_count = 0\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    mean_rewards = []\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state = preprocess_observation(state)\n",
    "\n",
    "    print(\"Rellenando replay buffer con acciones aleatorias...\")\n",
    "    while len(replay) < WARMUP_STEPS:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state_p = preprocess_observation(next_state)\n",
    "        replay.push(state, action, reward, next_state_p, done)\n",
    "        state = next_state_p if not done else preprocess_observation(env.reset()[0])\n",
    "        if done:\n",
    "            state = preprocess_observation(env.reset()[0])\n",
    "\n",
    "    print(f\"Replay buffer listo: {len(replay)} transiciones. Comenzando entrenamiento...\")\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_idx = 0\n",
    "    eps = EPS_START\n",
    "\n",
    "    while step_count < MAX_STEPS:\n",
    "        eps = max(eps*EPS_DECAY, EPS_END)\n",
    "\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_v = to_tensor(np.expand_dims(state, axis=0))\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state_v)\n",
    "                action = int(q_vals.argmax().item())\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state_p = preprocess_observation(next_state)\n",
    "        replay.push(state, action, reward, next_state_p, done)\n",
    "\n",
    "        state = next_state_p\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        if step_count > WARMUP_STEPS and step_count % UPDATE_FREQ == 0:\n",
    "            loss = compute_td_loss(BATCH_SIZE)\n",
    "            losses.append(loss)\n",
    "\n",
    "        if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "            target_net.load_state_dict(deepcopy(policy_net.state_dict()))\n",
    "            print(f\"[Paso {step_count}] Copiado a target network.\")\n",
    "\n",
    "        # fin de episodio\n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_idx += 1\n",
    "            # media móvil de últimas 100 episodios\n",
    "            if len(episode_rewards) >= 1:\n",
    "                m = np.mean(episode_rewards[-100:])\n",
    "                mean_rewards.append(m)\n",
    "            else:\n",
    "                mean_rewards.append(episode_reward)\n",
    "            if episode_idx % 10 == 0:\n",
    "                print(f\"Episode {episode_idx} | Step {step_count} | Episodic reward: {episode_reward:.1f} | \"\n",
    "                    f\"eps={eps:.3f} | mean100={mean_rewards[-1]:.2f} | replay={len(replay)}\")\n",
    "            # reiniciar episodio\n",
    "            state, _ = env.reset()\n",
    "            state = preprocess_observation(state)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "        if step_count % EVAL_EVERY == 0 and step_count > 0:\n",
    "            eval_rewards = []\n",
    "            for _ in range(EVAL_EPISODES):\n",
    "                s, _ = env.reset()\n",
    "                s = preprocess_observation(s)\n",
    "                done_eval = False\n",
    "                ep_r = 0.0\n",
    "                while not done_eval:\n",
    "                    s_v = to_tensor(np.expand_dims(s, axis=0))\n",
    "                    with torch.no_grad():\n",
    "                        a = int(policy_net(s_v).argmax().item())\n",
    "                    s2, r, term, trunc, _ = env.step(a)\n",
    "                    done_eval = term or trunc\n",
    "                    s = preprocess_observation(s2)\n",
    "                    ep_r += r\n",
    "                eval_rewards.append(ep_r)\n",
    "            avg_eval = float(np.mean(eval_rewards))\n",
    "            print(f\"*** Eval at step {step_count}: avg reward over {EVAL_EPISODES} eps = {avg_eval:.2f}\")\n",
    "\n",
    "            ckpt = {\n",
    "                \"policy_state_dict\": policy_net.state_dict(),\n",
    "                \"target_state_dict\": target_net.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"step\": step_count,\n",
    "                \"episode\": episode_idx\n",
    "            }\n",
    "            torch.save(ckpt, os.path.join(SAVE_PATH, f\"checkpoint_{step_count}.pth\"))\n",
    "            print(f\"Checkpoint guardado en {SAVE_PATH}/checkpoint_{step_count}.pth\")\n",
    "\n",
    "    torch.save(policy_net.state_dict(), os.path.join(SAVE_PATH, \"dqn_final.pth\"))\n",
    "    print(\"Entrenamiento terminado. Modelo guardado en:\", os.path.join(SAVE_PATH, \"dqn_final.pth\"))\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(\"Recompensa episódica / media móvil\")\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Recompensa\")\n",
    "    plt.plot(episode_rewards, alpha=0.4, label=\"episodios\")\n",
    "    if len(mean_rewards) > 0:\n",
    "        plt.plot(mean_rewards, label=\"media móvil 100\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return True\n",
    "\n",
    "trained = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 21.0\n",
      "Saved GIF to ./output.gif\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def watch_agent(env, model, max_steps=10000):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    images = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        state_tensor = (\n",
    "            torch.tensor(state, dtype=torch.float32)\n",
    "            .unsqueeze(0)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = model(state_tensor)\n",
    "        action = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        frame = env.render()\n",
    "\n",
    "        # Convert to PIL image if needed\n",
    "        if isinstance(frame, np.ndarray):\n",
    "            frame = Image.fromarray(frame)\n",
    "\n",
    "        images.append(frame)\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    return images\n",
    "\n",
    "if not trained:\n",
    "    # Load trained policy\n",
    "    policy_net = DQN(4, env.action_space.n)\n",
    "    checkpoint = torch.load(\n",
    "        r\"C:\\Users\\alvar\\OneDrive\\Documentos\\Carrera\\4\\Paradigmas\\Proyecto\\Part1\\checkpoint_best.pth\",\n",
    "        map_location=device\n",
    "    )\n",
    "    policy_net.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "\n",
    "# Generate frames\n",
    "images = watch_agent(env, policy_net)\n",
    "\n",
    "# Save as GIF\n",
    "gif_file = \"./output.gif\"\n",
    "images[0].save(\n",
    "    gif_file,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    duration=60,\n",
    "    loop=0\n",
    ")\n",
    "\n",
    "print(f\"Saved GIF to {gif_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
